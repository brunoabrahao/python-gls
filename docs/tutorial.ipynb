{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# When Your Data Breaks the Rules: A Gentle Introduction to Generalized Least Squares\n",
    "\n",
    "**A tutorial for `python-gls` — the first Python library for GLS with learned correlation structures**\n",
    "\n",
    "---\n",
    "\n",
    "## Who Is This For?\n",
    "\n",
    "If you've taken an introductory statistics course and learned about linear regression (OLS), you're ready for this tutorial. We'll use a concrete, everyday example to show you *why* the standard regression you learned has a dangerous blind spot — and how Generalized Least Squares (GLS) fixes it.\n",
    "\n",
    "By the end, you'll understand:\n",
    "- Why Ordinary Least Squares (OLS) can give you **wrong answers** when your data has a natural grouping\n",
    "- How **correlation structures** capture real patterns in your data\n",
    "- Why **unstructured correlations** give you the most flexibility to let your data speak\n",
    "- How to use `python-gls` to fit these models in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Freshman GPA Problem\n",
    "\n",
    "Imagine you're the dean of a university, and you want to answer a simple question:\n",
    "\n",
    "> **Does participating in a study group improve students' GPAs?**\n",
    "\n",
    "You have data on 60 students tracked across 4 semesters (freshman year through sophomore year). For each student, at each semester, you recorded:\n",
    "- Their **GPA** that semester (the outcome you care about)\n",
    "- Whether they participated in a **study group** (yes/no)\n",
    "- Their **course load** — how many credit hours they took\n",
    "\n",
    "Your instinct from intro stats: run a linear regression. GPA is the dependent variable, study group participation and course load are the predictors. Easy, right?\n",
    "\n",
    "**Not so fast.** There's a hidden problem in this data that, if ignored, will give you the wrong answer — potentially leading the university to waste resources on an ineffective program, or worse, to cancel a program that actually works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. A Quick Refresher: What OLS Assumes\n",
    "\n",
    "In your intro stats course, you learned that Ordinary Least Squares (OLS) regression finds the line (or plane) that minimizes the sum of squared errors:\n",
    "\n",
    "$$y_i = \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i} + \\varepsilon_i$$\n",
    "\n",
    "where $\\varepsilon_i$ is the \"error\" or \"residual\" — the part of $y$ that your predictors can't explain.\n",
    "\n",
    "For the math to work and give you valid p-values and confidence intervals, OLS requires three key assumptions about these errors:\n",
    "\n",
    "| Assumption | Plain English | Formal |\n",
    "|---|---|---|\n",
    "| **Independence** | Knowing one student's error tells you nothing about another's | $\\text{Cov}(\\varepsilon_i, \\varepsilon_j) = 0$ for $i \\ne j$ |\n",
    "| **Constant variance** | The spread of errors is the same everywhere | $\\text{Var}(\\varepsilon_i) = \\sigma^2$ for all $i$ |\n",
    "| **Normality** | Errors follow a bell curve | $\\varepsilon_i \\sim N(0, \\sigma^2)$ |\n",
    "\n",
    "The **independence** assumption is the one that gets violated most often in real-world data — and it's the most dangerous to ignore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Why Our GPA Data Breaks the Rules\n",
    "\n",
    "Look at our data again. We have 60 students $\\times$ 4 semesters = 240 observations. OLS would treat all 240 observations as independent — as if each row in our spreadsheet came from a different, unrelated student.\n",
    "\n",
    "But that's obviously wrong. Consider:\n",
    "\n",
    "- **Student A** had a 3.8 GPA last semester. What's your best guess for their GPA this semester? Probably close to 3.8 — not the class average of 3.2.\n",
    "- **Student B** struggled with a 2.1 last semester. Their next semester's GPA will likely be closer to 2.1 than to 3.2.\n",
    "\n",
    "The four GPA measurements from the same student are **correlated**. They share something — that student's underlying ability, motivation, study habits, and life circumstances — that makes them more similar to each other than to a random student's GPA.\n",
    "\n",
    "As Snijders and Bosker (2012, Ch. 2) put it, this dependency between observations within groups is not just a statistical nuisance — it's an *interesting phenomenon* that reflects real structure in the world. Students within the same student are like pupils within the same school: they share a common environment that makes their outcomes correlated.\n",
    "\n",
    "### The Intraclass Correlation\n",
    "\n",
    "How correlated are observations within the same student? The **intraclass correlation coefficient** (ICC) quantifies this (Snijders & Bosker, 2012, Sec. 3.3):\n",
    "\n",
    "$$\\rho_I = \\frac{\\tau^2}{\\tau^2 + \\sigma^2}$$\n",
    "\n",
    "where $\\tau^2$ is the variance *between* students and $\\sigma^2$ is the variance *within* students across semesters. If $\\rho_I = 0.5$, it means that half of the total variation in GPA is due to stable differences between students.\n",
    "\n",
    "In educational data, the ICC is typically between 0.10 and 0.25 for students nested in schools (Hedges & Hedberg, 2007). For **repeated measures on the same person**, it's often much higher — frequently 0.4 to 0.7 — because a person is much more similar to themselves over time than to a random other person."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. What Goes Wrong When You Ignore Correlation\n",
    "\n",
    "\"OK,\" you might think, \"so the data points are correlated. Does it really matter for my regression?\"\n",
    "\n",
    "**Yes. A lot.** Here's what happens:\n",
    "\n",
    "### Your Standard Errors Are Wrong\n",
    "\n",
    "OLS computes standard errors under the assumption that all 240 observations provide independent information. But they don't — the 4 observations from the same student are partly redundant. The **effective sample size** is smaller than 240.\n",
    "\n",
    "The design effect formula (Snijders & Bosker, 2012, Sec. 3.4) tells us how much information we lose:\n",
    "\n",
    "$$\\text{design effect} = 1 + (n - 1) \\cdot \\rho_I$$\n",
    "\n",
    "where $n$ is the group size (4 semesters) and $\\rho_I$ is the ICC. If $\\rho_I = 0.5$:\n",
    "\n",
    "$$\\text{design effect} = 1 + (4 - 1) \\times 0.5 = 2.5$$\n",
    "\n",
    "This means our 240 observations carry only as much information as $240 / 2.5 = 96$ truly independent observations. OLS *thinks* it has 240 independent data points, so it computes standard errors that are **too small** — making effects look more \"statistically significant\" than they really are.\n",
    "\n",
    "### You Find Effects That Don't Exist\n",
    "\n",
    "Because the standard errors are too small, your p-values are too small, and your confidence intervals are too narrow. You'll reject the null hypothesis too often — a problem called **inflated Type I error** (Dorman, 2008). You might conclude that the study group program \"works\" when it doesn't, or that a drug is effective when it isn't.\n",
    "\n",
    "### You Miss the Structure in Your Data\n",
    "\n",
    "Perhaps even more importantly, the correlation between repeated measurements is **informative**. It tells you something about the underlying process. Does a student's GPA this semester depend on last semester's GPA? How strong is that carryover? These are interesting questions that OLS simply ignores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. A Simulation\n",
    "\n",
    "Time to see the problem with actual code. We'll generate data where the within-student correlation has a **non-monotonic** pattern that no simple structure can capture.\n",
    "\n",
    "Here's the idea. A student's GPA in one semester is correlated with their GPA in other semesters, but not uniformly:\n",
    "\n",
    "- **Semesters 1–2** are highly correlated ($r = 0.70$) because students are still adjusting to college\n",
    "- **Semesters 3–4** are highly correlated ($r = 0.65$) once students have settled into their major\n",
    "- **Semesters 2–3** are weakly correlated ($r = 0.30$), perhaps because students changed majors or went abroad\n",
    "\n",
    "This pattern is non-monotonic: correlation doesn't simply decay with time distance. AR(1) and compound symmetry can't capture it. Only the unstructured model can."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-23T14:23:41.193447Z",
     "iopub.status.busy": "2026-02-23T14:23:41.193232Z",
     "iopub.status.idle": "2026-02-23T14:23:41.546664Z",
     "shell.execute_reply": "2026-02-23T14:23:41.546190Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True correlation matrix (the pattern we want to recover):\n",
      "        S1     S2     S3     S4\n",
      "S1    1.00   0.70   0.25   0.20\n",
      "S2    0.70   1.00   0.30   0.35\n",
      "S3    0.25   0.30   1.00   0.65\n",
      "S4    0.20   0.35   0.65   1.00\n",
      "\n",
      "Note: adjacent-semester correlations are 0.70, 0.30, 0.65\n",
      "      AR(1) would force these to be equal. They're not.\n",
      "\n",
      "Dataset: 240 observations from 60 students over 4 semesters\n",
      "\n",
      "First student's data:\n",
      " student  semester      gpa  study_group  course_load\n",
      "       0         1 2.634032            0     0.645269\n",
      "       0         2 3.365471            0    -1.976855\n",
      "       0         3 3.147840            0    -2.609690\n",
      "       0         4 2.401775            0     2.693313\n",
      "\n",
      "Second student's data:\n",
      " student  semester      gpa  study_group  course_load\n",
      "       1         1 3.113614            1     2.793792\n",
      "       1         2 3.093689            1     1.850384\n",
      "       1         3 3.033765            1    -1.172317\n",
      "       1         4 2.793780            1    -2.413967\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Study design\n",
    "n_students = 60\n",
    "n_semesters = 4\n",
    "N = n_students * n_semesters\n",
    "\n",
    "# Student-level variables\n",
    "student_id = np.repeat(np.arange(n_students), n_semesters)\n",
    "semester = np.tile(np.arange(1, n_semesters + 1), n_students)\n",
    "\n",
    "# Study group: assigned at student level (doesn't change across semesters)\n",
    "study_group = np.random.choice([0, 1], size=n_students)\n",
    "study_group_repeated = study_group[student_id]\n",
    "\n",
    "# Course load varies by semester (12-18 credits)\n",
    "course_load = np.random.uniform(12, 18, size=N)\n",
    "\n",
    "# --- The TRUE data-generating process ---\n",
    "beta_intercept = 3.0\n",
    "beta_study_group = 0.15   # true treatment effect\n",
    "beta_course_load = -0.04  # heavier load -> slightly lower GPA\n",
    "sigma_true = 0.3          # residual standard deviation\n",
    "\n",
    "# TRUE correlation: unstructured, non-monotonic (see Section 5 above)\n",
    "R_true = np.array([\n",
    "    [1.00, 0.70, 0.25, 0.20],\n",
    "    [0.70, 1.00, 0.30, 0.35],\n",
    "    [0.25, 0.30, 1.00, 0.65],\n",
    "    [0.20, 0.35, 0.65, 1.00],\n",
    "])\n",
    "Sigma_true = sigma_true**2 * R_true\n",
    "\n",
    "print(\"True correlation matrix (the pattern we want to recover):\")\n",
    "labels = ['S1', 'S2', 'S3', 'S4']\n",
    "print(f\"{'':4s} \" + \"  \".join(f\"{l:>5s}\" for l in labels))\n",
    "for i, l in enumerate(labels):\n",
    "    print(f\"{l:4s} \" + \"  \".join(f\"{R_true[i,j]:5.2f}\" for j in range(4)))\n",
    "print(f\"\\nNote: adjacent-semester correlations are 0.70, 0.30, 0.65\")\n",
    "print(f\"      AR(1) would force these to be equal. They're not.\")\n",
    "\n",
    "# Generate correlated errors for each student\n",
    "errors = np.zeros(N)\n",
    "for s in range(n_students):\n",
    "    idx = slice(s * n_semesters, (s + 1) * n_semesters)\n",
    "    errors[idx] = np.random.multivariate_normal(np.zeros(n_semesters), Sigma_true)\n",
    "\n",
    "# Generate GPA\n",
    "gpa = (beta_intercept\n",
    "       + beta_study_group * study_group_repeated\n",
    "       + beta_course_load * (course_load - 15)  # centered at 15 credits\n",
    "       + errors)\n",
    "\n",
    "# Clip to valid GPA range\n",
    "gpa = np.clip(gpa, 0, 4.0)\n",
    "\n",
    "# Build DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'gpa': gpa,\n",
    "    'study_group': study_group_repeated,\n",
    "    'course_load': course_load - 15,  # centered\n",
    "    'student': student_id,\n",
    "    'semester': semester,\n",
    "})\n",
    "\n",
    "print(f\"\\nDataset: {N} observations from {n_students} students over {n_semesters} semesters\")\n",
    "print(f\"\\nFirst student's data:\")\n",
    "print(df[df['student'] == 0][['student', 'semester', 'gpa', 'study_group', 'course_load']].to_string(index=False))\n",
    "print(f\"\\nSecond student's data:\")\n",
    "print(df[df['student'] == 1][['student', 'semester', 'gpa', 'study_group', 'course_load']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how each student's GPA values cluster together — they don't jump randomly from 2.0 to 4.0 between semesters. That's the correlation at work.\n",
    "\n",
    "Now let's fit OLS and see what happens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-23T14:23:41.564081Z",
     "iopub.status.busy": "2026-02-23T14:23:41.563947Z",
     "iopub.status.idle": "2026-02-23T14:23:41.962170Z",
     "shell.execute_reply": "2026-02-23T14:23:41.960964Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OLS Results (IGNORING within-student correlation)\n",
      "=======================================================\n",
      "==============================================================================\n",
      "                      Generalized Least Squares Results                       \n",
      "==============================================================================\n",
      "Method:          REML                 Log-Likelihood:         -65.0685\n",
      "No. Observations:240                  AIC:                    138.1371\n",
      "Df Model:        2                    BIC:                    152.0596\n",
      "Df Residuals:    237                  Sigma^2:                0.094690\n",
      "Converged:       Yes                  Iterations:                    0\n",
      "------------------------------------------------------------------------------\n",
      "                           coef    std err          t      P>|t|     [0.025     0.975]\n",
      "------------------------------------------------------------------------------\n",
      "           Intercept     3.0048     0.0286   105.1047     0.0000     2.9484     3.0611\n",
      "         study_group     0.1056     0.0398     2.6545     0.0085     0.0272     0.1840\n",
      "         course_load    -0.0381     0.0112    -3.4109     0.0008    -0.0602    -0.0161\n",
      "==============================================================================\n",
      "\n",
      "True study group effect: 0.15\n",
      "OLS estimate:            0.1056\n",
      "OLS std error:           0.0398\n",
      "OLS p-value:             0.0085\n"
     ]
    }
   ],
   "source": [
    "from python_gls import GLS\n",
    "\n",
    "# Fit OLS (ignoring the correlation between semesters for the same student)\n",
    "result_ols = GLS.from_formula(\n",
    "    \"gpa ~ study_group + course_load\",\n",
    "    data=df\n",
    ").fit()\n",
    "\n",
    "print(\"OLS Results (IGNORING within-student correlation)\")\n",
    "print(\"=\" * 55)\n",
    "print(result_ols.summary())\n",
    "print(f\"\\nTrue study group effect: {beta_study_group}\")\n",
    "print(f\"OLS estimate:            {result_ols.params['study_group']:.4f}\")\n",
    "print(f\"OLS std error:           {result_ols.bse['study_group']:.4f}\")\n",
    "print(f\"OLS p-value:             {result_ols.pvalues['study_group']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OLS reports standard errors as if all 240 observations were independent. But they're not — 4 observations come from each of 60 students, and observations within a student are correlated.\n",
    "\n",
    "A common first attempt: fit GLS with AR(1), which assumes correlation decays smoothly with time distance. It's better than OLS, but it's the wrong structure for our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-23T14:23:41.965390Z",
     "iopub.status.busy": "2026-02-23T14:23:41.965026Z",
     "iopub.status.idle": "2026-02-23T14:23:42.011756Z",
     "shell.execute_reply": "2026-02-23T14:23:42.008227Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GLS Results with AR(1) correlation\n",
      "=======================================================\n",
      "==============================================================================\n",
      "                      Generalized Least Squares Results                       \n",
      "==============================================================================\n",
      "Method:          REML                 Log-Likelihood:         -27.5374\n",
      "No. Observations:240                  AIC:                     65.0748\n",
      "Df Model:        2                    BIC:                     82.4780\n",
      "Df Residuals:    237                  Sigma^2:                0.096010\n",
      "Converged:       Yes                  Iterations:                    5\n",
      "------------------------------------------------------------------------------\n",
      "                           coef    std err          t      P>|t|     [0.025     0.975]\n",
      "------------------------------------------------------------------------------\n",
      "           Intercept     2.9955     0.0432    69.2737     0.0000     2.9103     3.0807\n",
      "         study_group     0.1136     0.0602     1.8885     0.0602    -0.0049     0.2321\n",
      "         course_load    -0.0385     0.0086    -4.4897     0.0000    -0.0554    -0.0216\n",
      "==============================================================================\n",
      "Correlation Structure: CorAR1\n",
      "  Parameters: [0.59074567]\n",
      "\n",
      "AR(1) estimated phi: 0.591\n",
      "  Implied correlations: lag 1 = 0.591, lag 2 = 0.349, lag 3 = 0.206\n",
      "\n",
      "But the TRUE adjacent correlations are 0.70, 0.30, 0.65 — not equal!\n",
      "AR(1) can only fit a single decay rate. It compromises.\n"
     ]
    }
   ],
   "source": [
    "from python_gls import GLS\n",
    "from python_gls.correlation import CorAR1\n",
    "\n",
    "# Fit GLS with AR(1) — a common default for longitudinal data\n",
    "result_gls_ar1 = GLS.from_formula(\n",
    "    \"gpa ~ study_group + course_load\",\n",
    "    data=df,\n",
    "    correlation=CorAR1(),   # Assumes correlation decays with time lag\n",
    "    groups=\"student\",       # Each student is an independent cluster\n",
    ").fit()\n",
    "\n",
    "print(\"GLS Results with AR(1) correlation\")\n",
    "print(\"=\" * 55)\n",
    "print(result_gls_ar1.summary())\n",
    "print(f\"\\nAR(1) estimated phi: {result_gls_ar1.correlation_params[0]:.3f}\")\n",
    "print(f\"  Implied correlations: lag 1 = {result_gls_ar1.correlation_params[0]:.3f}, \"\n",
    "      f\"lag 2 = {result_gls_ar1.correlation_params[0]**2:.3f}, \"\n",
    "      f\"lag 3 = {result_gls_ar1.correlation_params[0]**3:.3f}\")\n",
    "print(f\"\\nBut the TRUE adjacent correlations are 0.70, 0.30, 0.65 — not equal!\")\n",
    "print(f\"AR(1) can only fit a single decay rate. It compromises.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OLS vs GLS (AR1): A First Comparison\n",
    "\n",
    "AR(1) improves on OLS — the standard errors are more honest and AIC is lower. But it's fitting the wrong correlation structure to our data. We'll see in Section 7 that the unstructured model does better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-23T14:23:42.014931Z",
     "iopub.status.busy": "2026-02-23T14:23:42.014738Z",
     "iopub.status.idle": "2026-02-23T14:23:42.024254Z",
     "shell.execute_reply": "2026-02-23T14:23:42.022778Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        OLS    GLS (AR1)        Truth\n",
      "--------------------------------------------------------------------\n",
      "study_group coefficient              0.1056       0.1136       0.1500\n",
      "study_group std error                0.0398       0.0602             \n",
      "study_group p-value                  0.0085       0.0602             \n",
      "course_load coefficient             -0.0381      -0.0385      -0.0400\n",
      "AIC                                   138.1         65.1             \n",
      "\n",
      "GLS standard errors are larger (more honest) because they account\n",
      "for the fact that correlated observations carry less information.\n",
      "\n",
      "But AR(1) is the WRONG structure here. Can we do better?\n"
     ]
    }
   ],
   "source": [
    "print(f\"{'':30s} {'OLS':>12s} {'GLS (AR1)':>12s} {'Truth':>12s}\")\n",
    "print(\"-\" * 68)\n",
    "print(f\"{'study_group coefficient':30s} {result_ols.params['study_group']:12.4f} {result_gls_ar1.params['study_group']:12.4f} {beta_study_group:12.4f}\")\n",
    "print(f\"{'study_group std error':30s} {result_ols.bse['study_group']:12.4f} {result_gls_ar1.bse['study_group']:12.4f} {'':>12s}\")\n",
    "print(f\"{'study_group p-value':30s} {result_ols.pvalues['study_group']:12.4f} {result_gls_ar1.pvalues['study_group']:12.4f} {'':>12s}\")\n",
    "print(f\"{'course_load coefficient':30s} {result_ols.params['course_load']:12.4f} {result_gls_ar1.params['course_load']:12.4f} {beta_course_load:12.4f}\")\n",
    "print(f\"{'AIC':30s} {result_ols.aic:12.1f} {result_gls_ar1.aic:12.1f} {'':>12s}\")\n",
    "print(f\"\\nGLS standard errors are larger (more honest) because they account\")\n",
    "print(f\"for the fact that correlated observations carry less information.\")\n",
    "print(f\"\\nBut AR(1) is the WRONG structure here. Can we do better?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What we've seen so far:**\n",
    "\n",
    "1. **GLS with AR(1) is better than OLS** — larger, more honest standard errors and lower AIC.\n",
    "2. **But AR(1) is the wrong structure.** It forces adjacent correlations to be equal (all $\\phi$), while our data has correlations of 0.70, 0.30, and 0.65 between adjacent semesters.\n",
    "3. **We need a model that can recover the true non-monotonic pattern.** That's the unstructured model (`CorSymm`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Correlation Structures: How Data Points Talk to Each Other\n",
    "\n",
    "The key insight of GLS is that the correlation between observations is not a nuisance to be ignored — it's a **structure to be modeled**. Different structures encode different beliefs about *how* observations within a group relate to each other.\n",
    "\n",
    "### Compound Symmetry (Exchangeable)\n",
    "\n",
    "The simplest structure: any two observations from the same student are equally correlated, regardless of how far apart in time they are.\n",
    "\n",
    "$$\\text{Cor}(Y_{i,t}, Y_{i,s}) = \\rho \\quad \\text{for all } t \\neq s$$\n",
    "\n",
    "The correlation matrix for 4 semesters looks like:\n",
    "\n",
    "$$R = \\begin{pmatrix} 1 & \\rho & \\rho & \\rho \\\\ \\rho & 1 & \\rho & \\rho \\\\ \\rho & \\rho & 1 & \\rho \\\\ \\rho & \\rho & \\rho & 1 \\end{pmatrix}$$\n",
    "\n",
    "This says: your GPA in semester 1 is just as correlated with semester 4 as with semester 2. That might be reasonable for some data (e.g., test-retest reliability), but for longitudinal data it's often too restrictive — nearby time points are usually more correlated than distant ones (Snijders & Bosker, 2012, Sec. 15.1.1).\n",
    "\n",
    "### AR(1): Autoregressive\n",
    "\n",
    "A more realistic structure for time-ordered data: correlation decays with distance.\n",
    "\n",
    "$$\\text{Cor}(Y_{i,t}, Y_{i,s}) = \\phi^{|t - s|}$$\n",
    "\n",
    "With $\\phi = 0.6$ and 4 semesters:\n",
    "\n",
    "$$R = \\begin{pmatrix} 1 & 0.6 & 0.36 & 0.216 \\\\ 0.6 & 1 & 0.6 & 0.36 \\\\ 0.36 & 0.6 & 1 & 0.6 \\\\ 0.216 & 0.36 & 0.6 & 1 \\end{pmatrix}$$\n",
    "\n",
    "Adjacent semesters have correlation 0.6, but semesters 1 and 4 have correlation only $0.6^3 = 0.216$. This captures the intuition that recent semesters are more predictive of your current GPA than distant ones.\n",
    "\n",
    "As Snijders and Bosker (2012, Sec. 15.3) describe, autocorrelated residuals arise naturally in longitudinal data. When the random intercept and slope model doesn't fully capture the within-individual dynamics, the level-one residuals often show a pattern where a positive deviation in one period tends to be followed by another positive deviation — exactly the AR(1) pattern.\n",
    "\n",
    "### Unstructured: Let the Data Decide\n",
    "\n",
    "What if the true correlation doesn't follow any simple pattern? Maybe semesters 2 and 3 are highly correlated (both fall/spring of the same academic year) but semesters 1 and 3 are weakly correlated (different academic years). A rigid structure like AR(1) can't capture this.\n",
    "\n",
    "The **unstructured** (or **general**) correlation matrix estimates every pairwise correlation freely:\n",
    "\n",
    "$$R = \\begin{pmatrix} 1 & r_{12} & r_{13} & r_{14} \\\\ r_{12} & 1 & r_{23} & r_{24} \\\\ r_{13} & r_{23} & 1 & r_{34} \\\\ r_{14} & r_{24} & r_{34} & 1 \\end{pmatrix}$$\n",
    "\n",
    "Each $r_{ts}$ is estimated separately from the data. This is the most flexible approach — no pattern is assumed. The trade-off is that it requires estimating $d(d-1)/2$ parameters (6 for 4 time points), which means you need enough data.\n",
    "\n",
    "This is what Snijders and Bosker (2012, Sec. 15.1.3) call the **fully multivariate model**, where the covariance matrix is left \"completely free.\" It serves as a benchmark: if a simpler structure (like AR(1) or compound symmetry) fits almost as well, prefer the simpler model. If not, the unstructured model reveals patterns that simpler models miss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. The Right Structure: Letting the Data Speak\n",
    "\n",
    "Our data has a non-monotonic correlation pattern that neither compound symmetry nor AR(1) can capture. Only the unstructured model (`CorSymm`) can fit all six pairwise correlations freely. Let's compare all three."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-23T14:23:42.027079Z",
     "iopub.status.busy": "2026-02-23T14:23:42.026897Z",
     "iopub.status.idle": "2026-02-23T14:23:42.318668Z",
     "shell.execute_reply": "2026-02-23T14:23:42.318177Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Comparison\n",
      "===========================================================================\n",
      "Model                            AIC        BIC    Log-Lik     Corr Params\n",
      "---------------------------------------------------------------------------\n",
      "OLS (none)                     138.1      152.1      -65.1               0\n",
      "Compound Symmetry               69.3       86.7      -29.7         1 (rho)\n",
      "AR(1)                           65.1       82.5      -27.5         1 (phi)\n",
      "Unstructured (CorSymm)          52.1       86.9      -16.1        6 (r_ij)\n",
      "\n",
      "Lowest AIC wins. The unstructured model should win here because the\n",
      "true correlation is non-monotonic and can't be captured by AR(1) or CS.\n"
     ]
    }
   ],
   "source": [
    "from python_gls.correlation import CorAR1, CorCompSymm, CorSymm\n",
    "\n",
    "# Fit three models with different correlation structures\n",
    "\n",
    "# 1. Compound Symmetry: one parameter (rho), all pairs equal\n",
    "result_cs = GLS.from_formula(\n",
    "    \"gpa ~ study_group + course_load\",\n",
    "    data=df,\n",
    "    correlation=CorCompSymm(),\n",
    "    groups=\"student\",\n",
    ").fit()\n",
    "\n",
    "# 2. AR(1): one parameter (phi), correlation decays with lag\n",
    "result_ar1 = GLS.from_formula(\n",
    "    \"gpa ~ study_group + course_load\",\n",
    "    data=df,\n",
    "    correlation=CorAR1(),\n",
    "    groups=\"student\",\n",
    ").fit()\n",
    "\n",
    "# 3. Unstructured (CorSymm): d(d-1)/2 = 6 free parameters\n",
    "result_sym = GLS.from_formula(\n",
    "    \"gpa ~ study_group + course_load\",\n",
    "    data=df,\n",
    "    correlation=CorSymm(),\n",
    "    groups=\"student\",\n",
    ").fit()\n",
    "\n",
    "print(\"Model Comparison\")\n",
    "print(\"=\" * 75)\n",
    "print(f\"{'Model':25s} {'AIC':>10s} {'BIC':>10s} {'Log-Lik':>10s} {'Corr Params':>15s}\")\n",
    "print(\"-\" * 75)\n",
    "print(f\"{'OLS (none)':25s} {result_ols.aic:10.1f} {result_ols.bic:10.1f} {result_ols.loglik:10.1f} {'0':>15s}\")\n",
    "print(f\"{'Compound Symmetry':25s} {result_cs.aic:10.1f} {result_cs.bic:10.1f} {result_cs.loglik:10.1f} {'1 (rho)':>15s}\")\n",
    "print(f\"{'AR(1)':25s} {result_ar1.aic:10.1f} {result_ar1.bic:10.1f} {result_ar1.loglik:10.1f} {'1 (phi)':>15s}\")\n",
    "print(f\"{'Unstructured (CorSymm)':25s} {result_sym.aic:10.1f} {result_sym.bic:10.1f} {result_sym.loglik:10.1f} {'6 (r_ij)':>15s}\")\n",
    "print(f\"\\nLowest AIC wins. The unstructured model should win here because the\")\n",
    "print(f\"true correlation is non-monotonic and can't be captured by AR(1) or CS.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-23T14:23:42.320180Z",
     "iopub.status.busy": "2026-02-23T14:23:42.320072Z",
     "iopub.status.idle": "2026-02-23T14:23:42.325102Z",
     "shell.execute_reply": "2026-02-23T14:23:42.324595Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PARAMETER NAMES AND ESTIMATES BY STRUCTURE\n",
      "======================================================================\n",
      "\n",
      "1. Compound Symmetry (CorCompSymm)\n",
      "   Parameter: rho = 0.508\n",
      "   Meaning: ALL pairs have correlation 0.508\n",
      "   Implied correlation matrix:\n",
      "        S1    S2    S3    S4\n",
      "   S1  1.00  0.51  0.51  0.51\n",
      "   S2  0.51  1.00  0.51  0.51\n",
      "   S3  0.51  0.51  1.00  0.51\n",
      "   S4  0.51  0.51  0.51  1.00\n",
      "\n",
      "2. AR(1) (CorAR1)\n",
      "   Parameter: phi = 0.591\n",
      "   Meaning: corr(t, s) = phi^|t-s|, correlation decays with distance\n",
      "   Implied correlation matrix:\n",
      "        S1    S2    S3    S4\n",
      "   S1  1.00  0.59  0.35  0.21\n",
      "   S2  0.59  1.00  0.59  0.35\n",
      "   S3  0.35  0.59  1.00  0.59\n",
      "   S4  0.21  0.35  0.59  1.00\n",
      "\n",
      "3. Unstructured (CorSymm)\n",
      "   Parameters: 6 free pairwise correlations\n",
      "   Estimated correlation matrix:\n",
      "        S1    S2    S3    S4\n",
      "   S1  1.00  0.69  0.40  0.35\n",
      "   S2  0.69  1.00  0.40  0.54\n",
      "   S3  0.40  0.40  1.00  0.67\n",
      "   S4  0.35  0.54  0.67  1.00\n",
      "\n",
      "======================================================================\n",
      "ESTIMATED vs TRUE CORRELATIONS\n",
      "======================================================================\n",
      "      Pair     True       CS    AR(1)  CorSymm\n",
      "---------- -------- -------- -------- --------\n",
      "     S1-S2    0.700    0.508    0.591    0.690\n",
      "     S1-S3    0.250    0.508    0.349    0.402\n",
      "     S1-S4    0.200    0.508    0.206    0.353\n",
      "     S2-S3    0.300    0.508    0.591    0.397\n",
      "     S2-S4    0.350    0.508    0.349    0.542\n",
      "     S3-S4    0.650    0.508    0.591    0.673\n",
      "\n",
      "Only CorSymm can recover the non-monotonic pattern:\n",
      "  S1-S2 = 0.70 (high), S2-S3 = 0.30 (low), S3-S4 = 0.65 (high)\n"
     ]
    }
   ],
   "source": [
    "# What parameters does each structure estimate?\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PARAMETER NAMES AND ESTIMATES BY STRUCTURE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# --- Compound Symmetry ---\n",
    "rho = result_cs.correlation_params[0]\n",
    "print(f\"\\n1. Compound Symmetry (CorCompSymm)\")\n",
    "print(f\"   Parameter: rho = {rho:.3f}\")\n",
    "print(f\"   Meaning: ALL pairs have correlation {rho:.3f}\")\n",
    "print(f\"   Implied correlation matrix:\")\n",
    "print(f\"        S1    S2    S3    S4\")\n",
    "for i, l in enumerate(['S1','S2','S3','S4']):\n",
    "    row = [1.0 if i==j else rho for j in range(4)]\n",
    "    print(f\"   {l}  \" + \"  \".join(f\"{v:.2f}\" for v in row))\n",
    "\n",
    "# --- AR(1) ---\n",
    "phi = result_ar1.correlation_params[0]\n",
    "print(f\"\\n2. AR(1) (CorAR1)\")\n",
    "print(f\"   Parameter: phi = {phi:.3f}\")\n",
    "print(f\"   Meaning: corr(t, s) = phi^|t-s|, correlation decays with distance\")\n",
    "print(f\"   Implied correlation matrix:\")\n",
    "print(f\"        S1    S2    S3    S4\")\n",
    "for i, l in enumerate(['S1','S2','S3','S4']):\n",
    "    row = [phi**abs(i-j) for j in range(4)]\n",
    "    print(f\"   {l}  \" + \"  \".join(f\"{v:.2f}\" for v in row))\n",
    "\n",
    "# --- Unstructured ---\n",
    "print(f\"\\n3. Unstructured (CorSymm)\")\n",
    "print(f\"   Parameters: {n_semesters*(n_semesters-1)//2} free pairwise correlations\")\n",
    "R_est = result_sym.model.correlation.get_correlation_matrix(n_semesters)\n",
    "print(f\"   Estimated correlation matrix:\")\n",
    "print(f\"        S1    S2    S3    S4\")\n",
    "for i, l in enumerate(['S1','S2','S3','S4']):\n",
    "    print(f\"   {l}  \" + \"  \".join(f\"{R_est[i,j]:.2f}\" for j in range(4)))\n",
    "\n",
    "# --- Compare to truth ---\n",
    "print(f\"\\n{'=' * 70}\")\n",
    "print(f\"ESTIMATED vs TRUE CORRELATIONS\")\n",
    "print(f\"{'=' * 70}\")\n",
    "print(f\"{'Pair':>10s} {'True':>8s} {'CS':>8s} {'AR(1)':>8s} {'CorSymm':>8s}\")\n",
    "print(f\"{'-'*10} {'-'*8} {'-'*8} {'-'*8} {'-'*8}\")\n",
    "pairs = [(0,1,'S1-S2'), (0,2,'S1-S3'), (0,3,'S1-S4'),\n",
    "         (1,2,'S2-S3'), (1,3,'S2-S4'), (2,3,'S3-S4')]\n",
    "for i, j, label in pairs:\n",
    "    true_r = R_true[i,j]\n",
    "    cs_r = rho\n",
    "    ar_r = phi**abs(i-j)\n",
    "    sym_r = R_est[i,j]\n",
    "    print(f\"{label:>10s} {true_r:8.3f} {cs_r:8.3f} {ar_r:8.3f} {sym_r:8.3f}\")\n",
    "print(f\"\\nOnly CorSymm can recover the non-monotonic pattern:\")\n",
    "print(f\"  S1-S2 = 0.70 (high), S2-S3 = 0.30 (low), S3-S4 = 0.65 (high)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table above makes the point clearly:\n",
    "\n",
    "- **Compound Symmetry** (`rho`) forces all six correlations to be identical. It can't distinguish S1-S2 (0.70) from S2-S3 (0.30).\n",
    "- **AR(1)** (`phi`) forces correlations to decay monotonically with lag. It gets the overall level roughly right, but it can't capture the dip at S2-S3 followed by the rebound at S3-S4.\n",
    "- **Unstructured** (`CorSymm`) estimates each of the 6 pairwise correlations freely. It's the only structure that recovers the true non-monotonic pattern.\n",
    "\n",
    "The cost of this flexibility: 6 parameters instead of 1. AIC penalizes for this extra complexity, but if the true structure is genuinely irregular, CorSymm still wins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. The Math Behind GLS (Accessible Version)\n",
    "\n",
    "OLS finds coefficients by minimizing the sum of squared errors:\n",
    "\n",
    "$$\\hat{\\beta}_{\\text{OLS}} = (X'X)^{-1}X'y$$\n",
    "\n",
    "GLS does the same, but it **weights** the observations by the inverse of their covariance structure:\n",
    "\n",
    "$$\\hat{\\beta}_{\\text{GLS}} = (X'\\Omega^{-1}X)^{-1}X'\\Omega^{-1}y$$\n",
    "\n",
    "where $\\Omega$ is the covariance matrix that encodes the correlations. Think of it this way:\n",
    "- Observations that are highly correlated with others get **less weight** (they provide redundant information)\n",
    "- Observations that are relatively independent get **more weight** (they provide unique information)\n",
    "\n",
    "The covariance matrix has a specific structure (Snijders & Bosker, 2012, Ch. 15):\n",
    "\n",
    "$$\\Omega_g = \\sigma^2 \\cdot A_g^{1/2} \\, R_g \\, A_g^{1/2}$$\n",
    "\n",
    "where:\n",
    "- $R_g$ is the **correlation matrix** (AR(1), compound symmetry, unstructured, etc.)\n",
    "- $A_g$ is a diagonal matrix of **variance weights** (for heteroscedasticity)\n",
    "- $\\sigma^2$ is the overall residual variance\n",
    "\n",
    "The key innovation in GLS is that $R_g$ and $A_g$ are **estimated from the data** via maximum likelihood (ML) or restricted maximum likelihood (REML). You don't need to know the correlation in advance — the algorithm discovers it.\n",
    "\n",
    "### ML vs REML\n",
    "\n",
    "Two estimation methods are available:\n",
    "- **ML** (Maximum Likelihood): tends to underestimate variance (divides by $N$)\n",
    "- **REML** (Restricted Maximum Likelihood): corrects for the bias (divides by $N - k$), where $k$ is the number of fixed effects\n",
    "\n",
    "REML is the default in both R's `nlme::gls()` and in `python-gls`, and is generally preferred for estimating variance parameters (Snijders & Bosker, 2012, Sec. 4.7)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Beyond Correlation: Heteroscedasticity\n",
    "\n",
    "Sometimes the *variance* of the errors isn't constant either. For example:\n",
    "- GPA variability might be larger for students with heavier course loads (more ways to do well or poorly)\n",
    "- Measurement precision might differ between online and in-person semesters\n",
    "\n",
    "GLS can handle this too, by modeling the variance as a function of a covariate or group membership. `python-gls` provides six variance functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-23T14:23:42.326458Z",
     "iopub.status.busy": "2026-02-23T14:23:42.326375Z",
     "iopub.status.idle": "2026-02-23T14:23:42.346308Z",
     "shell.execute_reply": "2026-02-23T14:23:42.345864Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GLS with AR(1) + Group-specific Variance\n",
      "==================================================\n",
      "==============================================================================\n",
      "                      Generalized Least Squares Results                       \n",
      "==============================================================================\n",
      "Method:          REML                 Log-Likelihood:         -27.2646\n",
      "No. Observations:240                  AIC:                     66.5292\n",
      "Df Model:        2                    BIC:                     87.4130\n",
      "Df Residuals:    237                  Sigma^2:                0.103701\n",
      "Converged:       Yes                  Iterations:                    6\n",
      "------------------------------------------------------------------------------\n",
      "                           coef    std err          t      P>|t|     [0.025     0.975]\n",
      "------------------------------------------------------------------------------\n",
      "           Intercept     2.9953     0.0451    66.3899     0.0000     2.9064     3.0842\n",
      "         study_group     0.1139     0.0608     1.8732     0.0623    -0.0059     0.2336\n",
      "         course_load    -0.0378     0.0085    -4.4333     0.0000    -0.0546    -0.0210\n",
      "==============================================================================\n",
      "Correlation Structure: CorAR1\n",
      "  Parameters: [0.59664135]\n",
      "Variance Function: VarIdent\n",
      "  Parameters: [-0.06883964]\n",
      "\n",
      "Variance ratio parameter: [-0.06883964]\n",
      "(A value near 1.0 means variances are similar across groups)\n"
     ]
    }
   ],
   "source": [
    "from python_gls.variance import VarIdent\n",
    "\n",
    "# What if the variance differs between study group and non-study-group students?\n",
    "result_hetero = GLS.from_formula(\n",
    "    \"gpa ~ study_group + course_load\",\n",
    "    data=df,\n",
    "    correlation=CorAR1(),\n",
    "    variance=VarIdent(\"study_group\"),  # Allow different variance per group\n",
    "    groups=\"student\",\n",
    ").fit()\n",
    "\n",
    "print(\"GLS with AR(1) + Group-specific Variance\")\n",
    "print(\"=\" * 50)\n",
    "print(result_hetero.summary())\n",
    "print(f\"\\nVariance ratio parameter: {result_hetero.variance_params}\")\n",
    "print(f\"(A value near 1.0 means variances are similar across groups)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Introducing `python-gls`: GLS for Python, Finally\n",
    "\n",
    "### The Gap\n",
    "\n",
    "For over 25 years, R users have had `nlme::gls()` (Pinheiro & Bates, 2000) — a function that estimates correlation and variance structures from data automatically. It has been the workhorse for longitudinal data analysis in biostatistics, social science, and beyond.\n",
    "\n",
    "Python, despite its dominance in data science, has had **no equivalent**. The `statsmodels.GLS` class requires you to supply a pre-computed covariance matrix — you have to know $\\Omega$ in advance, which defeats the purpose. There has been no way to say \"I have AR(1) correlated errors; please estimate $\\phi$ for me.\"\n",
    "\n",
    "**`python-gls` fills this gap.** It is the first Python library to provide automatic estimation of correlation and variance structures via ML/REML, matching R's `nlme::gls()` behavior.\n",
    "\n",
    "### Features at a Glance\n",
    "\n",
    "| Feature | Details |\n",
    "|---|---|\n",
    "| **11 correlation structures** | AR(1), ARMA(p,q), continuous-time AR(1), compound symmetry, unstructured, exponential, Gaussian, linear, rational quadratic, spherical |\n",
    "| **6 variance functions** | Group-specific, power, exponential, constant-power, fixed, combined |\n",
    "| **Estimation** | ML and REML, with profile likelihood optimization |\n",
    "| **R-style formulas** | `\"y ~ x1 + C(group) * time\"` — same syntax you'd use in R |\n",
    "| **Parallel computation** | `n_jobs=-1` distributes group-level operations across CPU cores |\n",
    "| **Validated against R** | Comprehensive test suite comparing output to R's `nlme::gls()` |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance: Parallel GLS\n",
    "\n",
    "Real datasets can be large — thousands of groups, each with dozens of time points. `python-gls` includes several optimizations to make GLS fast:\n",
    "\n",
    "1. **Analytic inverses** for AR(1) and compound symmetry matrices — $O(m)$ instead of $O(m^3)$\n",
    "2. **Batched NumPy operations** for balanced panels — a single matrix multiply across all groups\n",
    "3. **Block-diagonal inversion** — $O(n \\cdot m^3)$ instead of $O(N^3)$\n",
    "4. **Thread-level parallelism** — pass `n_jobs=-1` to distribute work across CPU cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-23T14:23:42.347782Z",
     "iopub.status.busy": "2026-02-23T14:23:42.347685Z",
     "iopub.status.idle": "2026-02-23T14:23:42.486212Z",
     "shell.execute_reply": "2026-02-23T14:23:42.485700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 4000 observations, 500 groups, 8 time points\n",
      "Sequential (n_jobs=1):  0.07s\n",
      "Parallel   (n_jobs=-1): 0.07s\n",
      "Speedup: 1.0x\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Generate a larger dataset to see parallelism benefits\n",
    "np.random.seed(123)\n",
    "n_large = 500\n",
    "n_t = 8\n",
    "N_large = n_large * n_t\n",
    "\n",
    "df_large = pd.DataFrame({\n",
    "    'y': np.random.randn(N_large),\n",
    "    'x': np.random.randn(N_large),\n",
    "    'group': np.repeat(np.arange(n_large), n_t),\n",
    "})\n",
    "# Add a real signal\n",
    "df_large['y'] = 1.0 + 0.5 * df_large['x'] + df_large['y'] * 0.5\n",
    "\n",
    "# Sequential\n",
    "t0 = time.time()\n",
    "r_seq = GLS.from_formula(\"y ~ x\", data=df_large, correlation=CorAR1(), groups=\"group\").fit(n_jobs=1)\n",
    "t_seq = time.time() - t0\n",
    "\n",
    "# Parallel\n",
    "t0 = time.time()\n",
    "r_par = GLS.from_formula(\"y ~ x\", data=df_large, correlation=CorAR1(), groups=\"group\").fit(n_jobs=-1)\n",
    "t_par = time.time() - t0\n",
    "\n",
    "print(f\"Dataset: {N_large} observations, {n_large} groups, {n_t} time points\")\n",
    "print(f\"Sequential (n_jobs=1):  {t_seq:.2f}s\")\n",
    "print(f\"Parallel   (n_jobs=-1): {t_par:.2f}s\")\n",
    "print(f\"Speedup: {t_seq/t_par:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Putting It All Together: A Complete Workflow\n",
    "\n",
    "Here's the recommended workflow for analyzing longitudinal or clustered data with `python-gls`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-23T14:23:42.487638Z",
     "iopub.status.busy": "2026-02-23T14:23:42.487544Z",
     "iopub.status.idle": "2026-02-23T14:23:42.799476Z",
     "shell.execute_reply": "2026-02-23T14:23:42.798942Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3: Model Selection via AIC\n",
      "==================================================\n",
      "  Unstructured               AIC =     52.1  <-- best\n",
      "  AR(1)                      AIC =     65.1  \n",
      "  Compound Symmetry          AIC =     69.3  \n",
      "  OLS                        AIC =    138.1  \n",
      "\n",
      "Step 4: Results from best model (Unstructured)\n",
      "==================================================\n",
      "==============================================================================\n",
      "                      Generalized Least Squares Results                       \n",
      "==============================================================================\n",
      "Method:          REML                 Log-Likelihood:         -16.0718\n",
      "No. Observations:240                  AIC:                     52.1435\n",
      "Df Model:        2                    BIC:                     86.9499\n",
      "Df Residuals:    237                  Sigma^2:                0.096054\n",
      "Converged:       Yes                  Iterations:                    9\n",
      "------------------------------------------------------------------------------\n",
      "                           coef    std err          t      P>|t|     [0.025     0.975]\n",
      "------------------------------------------------------------------------------\n",
      "           Intercept     3.0074     0.0456    65.9124     0.0000     2.9175     3.0972\n",
      "         study_group     0.1013     0.0635     1.5962     0.1118    -0.0237     0.2264\n",
      "         course_load    -0.0484     0.0075    -6.4981     0.0000    -0.0631    -0.0337\n",
      "==============================================================================\n",
      "Correlation Structure: CorSymm\n",
      "  Parameters: [-1.05884114 -0.53884747 -0.23223111 -0.46771474 -0.59752742 -0.89266171]\n",
      "\n",
      "95% Confidence Intervals:\n",
      "                lower     upper\n",
      "Intercept    2.917467  3.097237\n",
      "study_group -0.023729  0.226388\n",
      "course_load -0.063088 -0.033734\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Start with OLS as a baseline\n",
    "result_ols = GLS.from_formula(\"gpa ~ study_group + course_load\", data=df).fit()\n",
    "\n",
    "# Step 2: Try different correlation structures\n",
    "result_cs  = GLS.from_formula(\"gpa ~ study_group + course_load\", data=df,\n",
    "                               correlation=CorCompSymm(), groups=\"student\").fit()\n",
    "result_ar1 = GLS.from_formula(\"gpa ~ study_group + course_load\", data=df,\n",
    "                               correlation=CorAR1(), groups=\"student\").fit()\n",
    "result_sym = GLS.from_formula(\"gpa ~ study_group + course_load\", data=df,\n",
    "                               correlation=CorSymm(), groups=\"student\").fit()\n",
    "\n",
    "# Step 3: Compare models using AIC (lower is better)\n",
    "models = [\n",
    "    (\"OLS\",                result_ols),\n",
    "    (\"Compound Symmetry\",  result_cs),\n",
    "    (\"AR(1)\",              result_ar1),\n",
    "    (\"Unstructured\",       result_sym),\n",
    "]\n",
    "\n",
    "print(\"Step 3: Model Selection via AIC\")\n",
    "print(\"=\" * 50)\n",
    "for name, r in sorted(models, key=lambda x: x[1].aic):\n",
    "    print(f\"  {name:25s}  AIC = {r.aic:8.1f}  {'<-- best' if r.aic == min(m[1].aic for m in models) else ''}\")\n",
    "\n",
    "# Step 4: Interpret the best model\n",
    "best_name, best_result = min(models, key=lambda x: x[1].aic)\n",
    "print(f\"\\nStep 4: Results from best model ({best_name})\")\n",
    "print(\"=\" * 50)\n",
    "print(best_result.summary())\n",
    "print(f\"\\n95% Confidence Intervals:\")\n",
    "print(best_result.conf_int())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Summary\n",
    "\n",
    "### What You've Learned\n",
    "\n",
    "1. **OLS assumes independence** — but real data (especially longitudinal or clustered data) violates this. Ignoring the violation leads to standard errors that are too small and conclusions that are unreliable.\n",
    "\n",
    "2. **Correlation structures** describe how observations within a group are related. Common choices include:\n",
    "   - **Compound symmetry**: all pairs equally correlated (simplest, most restrictive)\n",
    "   - **AR(1)**: correlation decays with time lag (natural for time series)\n",
    "   - **Unstructured**: every pair has its own correlation (most flexible, lets data speak)\n",
    "\n",
    "3. **Unstructured correlations** are especially valuable when the pattern of association between time points doesn't follow a simple formula. While mixed effects models (random intercepts/slopes) impose specific structures, GLS with `CorSymm` can capture arbitrary patterns.\n",
    "\n",
    "4. **GLS estimates these structures from your data** via maximum likelihood — you don't need to know the correlation in advance.\n",
    "\n",
    "5. **Model selection** (AIC/BIC) helps you choose the right level of complexity: start simple, and add complexity only when the data supports it.\n",
    "\n",
    "### `python-gls` in One Paragraph\n",
    "\n",
    "`python-gls` is the first Python library to provide automatic GLS estimation with learned correlation and variance structures — functionality that has been available in R through `nlme::gls()` (Pinheiro & Bates, 2000) for over 25 years but has been missing from the Python ecosystem. It supports 11 correlation structures (including unstructured), 6 variance functions, both ML and REML estimation, R-style formula syntax, and parallel computation for large datasets. It has been validated against R's `nlme::gls()` across 12 diverse test scenarios.\n",
    "\n",
    "Install it with:\n",
    "```bash\n",
    "pip install python-gls\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- Dorman, C. F. (2008). Effects of incorporating spatial autocorrelation into the analysis of species distribution data. *Global Ecology and Biogeography*, 16(2), 129–138.\n",
    "\n",
    "- Hedges, L. V., & Hedberg, E. C. (2007). Intraclass correlation values for planning group-randomized trials in education. *Educational Evaluation and Policy Analysis*, 29(1), 60–87.\n",
    "\n",
    "- Pinheiro, J. C., & Bates, D. M. (2000). *Mixed-Effects Models in S and S-PLUS*. Springer.\n",
    "\n",
    "- Pinheiro, J. C., & Bates, D. M. (1996). Unconstrained parametrizations for variance-covariance matrices. *Statistics and Computing*, 6(3), 289–296.\n",
    "\n",
    "- Snijders, T. A. B., & Bosker, R. J. (2012). *Multilevel Analysis: An Introduction to Basic and Advanced Multilevel Modeling* (2nd ed.). SAGE Publications.\n",
    "  - Ch. 2: Dependence as a nuisance vs. an interesting phenomenon\n",
    "  - Ch. 3, Sec. 3.3–3.4: Intraclass correlation and design effects\n",
    "  - Ch. 15, Sec. 15.1: Fixed occasion designs, compound symmetry\n",
    "  - Ch. 15, Sec. 15.1.3: The fully multivariate (unstructured) model\n",
    "  - Ch. 15, Sec. 15.3: Autocorrelated residuals"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
