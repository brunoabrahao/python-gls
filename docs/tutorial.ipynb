{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# When Your Data Breaks the Rules: A Gentle Introduction to Generalized Least Squares\n",
    "\n",
    "**A tutorial for `python-gls` — the first Python library for GLS with learned correlation structures**\n",
    "\n",
    "---\n",
    "\n",
    "## Who Is This For?\n",
    "\n",
    "If you've taken an introductory statistics course and learned about linear regression (OLS), you're ready for this tutorial. We'll use a concrete, everyday example to show you *why* the standard regression you learned has a dangerous blind spot — and how Generalized Least Squares (GLS) fixes it.\n",
    "\n",
    "By the end, you'll understand:\n",
    "- Why Ordinary Least Squares (OLS) can give you **wrong answers** when your data has a natural grouping\n",
    "- How **correlation structures** capture real patterns in your data\n",
    "- Why **unstructured correlations** give you the most flexibility to let your data speak\n",
    "- How to use `python-gls` to fit these models in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Freshman GPA Problem\n",
    "\n",
    "Imagine you're the dean of a university, and you want to answer a simple question:\n",
    "\n",
    "> **Does participating in a study group improve students' GPAs?**\n",
    "\n",
    "You have data on 60 students tracked across 4 semesters (freshman year through sophomore year). For each student, at each semester, you recorded:\n",
    "- Their **GPA** that semester (the outcome you care about)\n",
    "- Whether they participated in a **study group** (yes/no)\n",
    "- Their **course load** — how many credit hours they took\n",
    "\n",
    "Your instinct from intro stats: run a linear regression. GPA is the dependent variable, study group participation and course load are the predictors. Easy, right?\n",
    "\n",
    "**Not so fast.** There's a hidden problem in this data that, if ignored, will give you the wrong answer — potentially leading the university to waste resources on an ineffective program, or worse, to cancel a program that actually works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. A Quick Refresher: What OLS Assumes\n",
    "\n",
    "In your intro stats course, you learned that Ordinary Least Squares (OLS) regression finds the line (or plane) that minimizes the sum of squared errors:\n",
    "\n",
    "$$y_i = \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i} + \\varepsilon_i$$\n",
    "\n",
    "where $\\varepsilon_i$ is the \"error\" or \"residual\" — the part of $y$ that your predictors can't explain.\n",
    "\n",
    "For the math to work and give you valid p-values and confidence intervals, OLS requires three key assumptions about these errors:\n",
    "\n",
    "| Assumption | Plain English | Formal |\n",
    "|---|---|---|\n",
    "| **Independence** | Knowing one student's error tells you nothing about another's | $\\text{Cov}(\\varepsilon_i, \\varepsilon_j) = 0$ for $i \\ne j$ |\n",
    "| **Constant variance** | The spread of errors is the same everywhere | $\\text{Var}(\\varepsilon_i) = \\sigma^2$ for all $i$ |\n",
    "| **Normality** | Errors follow a bell curve | $\\varepsilon_i \\sim N(0, \\sigma^2)$ |\n",
    "\n",
    "The **independence** assumption is the one that gets violated most often in real-world data — and it's the most dangerous to ignore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Why Our GPA Data Breaks the Rules\n",
    "\n",
    "Look at our data again. We have 60 students $\\times$ 4 semesters = 240 observations. OLS would treat all 240 observations as independent — as if each row in our spreadsheet came from a different, unrelated student.\n",
    "\n",
    "But that's obviously wrong. Consider:\n",
    "\n",
    "- **Student A** had a 3.8 GPA last semester. What's your best guess for their GPA this semester? Probably close to 3.8 — not the class average of 3.2.\n",
    "- **Student B** struggled with a 2.1 last semester. Their next semester's GPA will likely be closer to 2.1 than to 3.2.\n",
    "\n",
    "The four GPA measurements from the same student are **correlated**. They share something — that student's underlying ability, motivation, study habits, and life circumstances — that makes them more similar to each other than to a random student's GPA.\n",
    "\n",
    "As Snijders and Bosker (2012, Ch. 2) put it, this dependency between observations within groups is not just a statistical nuisance — it's an *interesting phenomenon* that reflects real structure in the world. Students within the same student are like pupils within the same school: they share a common environment that makes their outcomes correlated.\n",
    "\n",
    "### The Intraclass Correlation\n",
    "\n",
    "How correlated are observations within the same student? The **intraclass correlation coefficient** (ICC) quantifies this (Snijders & Bosker, 2012, Sec. 3.3):\n",
    "\n",
    "$$\\rho_I = \\frac{\\tau^2}{\\tau^2 + \\sigma^2}$$\n",
    "\n",
    "where $\\tau^2$ is the variance *between* students and $\\sigma^2$ is the variance *within* students across semesters. If $\\rho_I = 0.5$, it means that half of the total variation in GPA is due to stable differences between students.\n",
    "\n",
    "In educational data, the ICC is typically between 0.10 and 0.25 for students nested in schools (Hedges & Hedberg, 2007). For **repeated measures on the same person**, it's often much higher — frequently 0.4 to 0.7 — because a person is much more similar to themselves over time than to a random other person."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. What Goes Wrong When You Ignore Correlation\n",
    "\n",
    "\"OK,\" you might think, \"so the data points are correlated. Does it really matter for my regression?\"\n",
    "\n",
    "**Yes. A lot.** Here's what happens:\n",
    "\n",
    "### Your Standard Errors Are Wrong\n",
    "\n",
    "OLS computes standard errors under the assumption that all 240 observations provide independent information. But they don't — the 4 observations from the same student are partly redundant. The **effective sample size** is smaller than 240.\n",
    "\n",
    "The design effect formula (Snijders & Bosker, 2012, Sec. 3.4) tells us how much information we lose:\n",
    "\n",
    "$$\\text{design effect} = 1 + (n - 1) \\cdot \\rho_I$$\n",
    "\n",
    "where $n$ is the group size (4 semesters) and $\\rho_I$ is the ICC. If $\\rho_I = 0.5$:\n",
    "\n",
    "$$\\text{design effect} = 1 + (4 - 1) \\times 0.5 = 2.5$$\n",
    "\n",
    "This means our 240 observations carry only as much information as $240 / 2.5 = 96$ truly independent observations. OLS *thinks* it has 240 independent data points, so it computes standard errors that are **too small** — making effects look more \"statistically significant\" than they really are.\n",
    "\n",
    "### You Find Effects That Don't Exist\n",
    "\n",
    "Because the standard errors are too small, your p-values are too small, and your confidence intervals are too narrow. You'll reject the null hypothesis too often — a problem called **inflated Type I error** (Dorman, 2008). You might conclude that the study group program \"works\" when it doesn't, or that a drug is effective when it isn't.\n",
    "\n",
    "### You Miss the Structure in Your Data\n",
    "\n",
    "Perhaps even more importantly, the correlation between repeated measurements is **informative**. It tells you something about the underlying process. Does a student's GPA this semester depend on last semester's GPA? How strong is that carryover? These are interesting questions that OLS simply ignores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Let's See It With Real Numbers\n",
    "\n",
    "Time to simulate this scenario and see the problem — and the solution — with actual code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Study design\n",
    "n_students = 60\n",
    "n_semesters = 4\n",
    "N = n_students * n_semesters\n",
    "\n",
    "# Student-level variables\n",
    "student_id = np.repeat(np.arange(n_students), n_semesters)\n",
    "semester = np.tile(np.arange(1, n_semesters + 1), n_students)\n",
    "\n",
    "# Study group: assigned at student level (doesn't change across semesters)\n",
    "study_group = np.random.choice([0, 1], size=n_students)\n",
    "study_group_repeated = study_group[student_id]\n",
    "\n",
    "# Course load varies by semester (12-18 credits)\n",
    "course_load = np.random.uniform(12, 18, size=N)\n",
    "\n",
    "# --- The TRUE data-generating process ---\n",
    "# True effect: study group adds 0.15 GPA points (a modest, real effect)\n",
    "# Course load has a small negative effect (heavier load -> slightly lower GPA)\n",
    "# AR(1) correlated errors within each student (phi = 0.6)\n",
    "\n",
    "beta_intercept = 3.0\n",
    "beta_study_group = 0.15   # true treatment effect\n",
    "beta_course_load = -0.04  # heavier load -> slightly lower GPA\n",
    "phi_true = 0.6            # autocorrelation: this semester's \"shock\" carries over\n",
    "sigma_true = 0.3          # residual standard deviation\n",
    "\n",
    "# Generate AR(1) correlated errors for each student\n",
    "errors = np.zeros(N)\n",
    "for s in range(n_students):\n",
    "    idx = slice(s * n_semesters, (s + 1) * n_semesters)\n",
    "    e = np.random.randn(n_semesters)\n",
    "    for t in range(1, n_semesters):\n",
    "        e[t] = phi_true * e[t-1] + np.sqrt(1 - phi_true**2) * e[t]\n",
    "    errors[idx] = sigma_true * e\n",
    "\n",
    "# Generate GPA\n",
    "gpa = (beta_intercept\n",
    "       + beta_study_group * study_group_repeated\n",
    "       + beta_course_load * (course_load - 15)  # centered at 15 credits\n",
    "       + errors)\n",
    "\n",
    "# Clip to valid GPA range\n",
    "gpa = np.clip(gpa, 0, 4.0)\n",
    "\n",
    "# Build DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'gpa': gpa,\n",
    "    'study_group': study_group_repeated,\n",
    "    'course_load': course_load - 15,  # centered\n",
    "    'student': student_id,\n",
    "    'semester': semester,\n",
    "})\n",
    "\n",
    "print(f\"Dataset: {N} observations from {n_students} students over {n_semesters} semesters\")\n",
    "print(f\"\\nFirst student's data (notice how GPAs are similar across semesters):\")\n",
    "print(df[df['student'] == 0][['student', 'semester', 'gpa', 'study_group', 'course_load']].to_string(index=False))\n",
    "print(f\"\\nSecond student's data:\")\n",
    "print(df[df['student'] == 1][['student', 'semester', 'gpa', 'study_group', 'course_load']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how each student's GPA values cluster together — they don't jump randomly from 2.0 to 4.0 between semesters. That's the correlation at work.\n",
    "\n",
    "Now let's fit OLS and see what happens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from python_gls import GLS\n",
    "\n",
    "# Fit OLS (ignoring the correlation between semesters for the same student)\n",
    "result_ols = GLS.from_formula(\n",
    "    \"gpa ~ study_group + course_load\",\n",
    "    data=df\n",
    ").fit()\n",
    "\n",
    "print(\"OLS Results (IGNORING within-student correlation)\")\n",
    "print(\"=\" * 55)\n",
    "print(result_ols.summary())\n",
    "print(f\"\\nTrue study group effect: {beta_study_group}\")\n",
    "print(f\"OLS estimate:            {result_ols.params['study_group']:.4f}\")\n",
    "print(f\"OLS std error:           {result_ols.bse['study_group']:.4f}\")\n",
    "print(f\"OLS p-value:             {result_ols.pvalues['study_group']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OLS reports standard errors as if all 240 observations were independent. But they're not — 4 observations come from each of 60 students, and observations within a student are correlated at $\\phi = 0.6$.\n",
    "\n",
    "Now let's fit the model **correctly**, accounting for the within-student correlation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from python_gls.correlation import CorAR1\n",
    "\n",
    "# Fit GLS with AR(1) correlation (the TRUE structure)\n",
    "result_gls = GLS.from_formula(\n",
    "    \"gpa ~ study_group + course_load\",\n",
    "    data=df,\n",
    "    correlation=CorAR1(),   # Learn the autocorrelation from data\n",
    "    groups=\"student\",       # Each student is an independent cluster\n",
    ").fit()\n",
    "\n",
    "print(\"GLS Results (ACCOUNTING for within-student AR(1) correlation)\")\n",
    "print(\"=\" * 63)\n",
    "print(result_gls.summary())\n",
    "print(f\"\\nEstimated autocorrelation (phi): {result_gls.correlation_params[0]:.3f}  (true: {phi_true})\")\n",
    "print(f\"\\nTrue study group effect: {beta_study_group}\")\n",
    "print(f\"GLS estimate:            {result_gls.params['study_group']:.4f}\")\n",
    "print(f\"GLS std error:           {result_gls.bse['study_group']:.4f}\")\n",
    "print(f\"GLS p-value:             {result_gls.pvalues['study_group']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing OLS vs GLS\n",
    "\n",
    "Let's put the results side by side:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{'':30s} {'OLS':>12s} {'GLS (AR1)':>12s} {'Truth':>12s}\")\n",
    "print(\"-\" * 68)\n",
    "print(f\"{'study_group coefficient':30s} {result_ols.params['study_group']:12.4f} {result_gls.params['study_group']:12.4f} {beta_study_group:12.4f}\")\n",
    "print(f\"{'study_group std error':30s} {result_ols.bse['study_group']:12.4f} {result_gls.bse['study_group']:12.4f} {'—':>12s}\")\n",
    "print(f\"{'study_group p-value':30s} {result_ols.pvalues['study_group']:12.4f} {result_gls.pvalues['study_group']:12.4f} {'—':>12s}\")\n",
    "print(f\"{'course_load coefficient':30s} {result_ols.params['course_load']:12.4f} {result_gls.params['course_load']:12.4f} {beta_course_load:12.4f}\")\n",
    "print(f\"{'AIC':30s} {result_ols.aic:12.1f} {result_gls.aic:12.1f} {'—':>12s}\")\n",
    "print(f\"{'Estimated phi':30s} {'—':>12s} {result_gls.correlation_params[0]:12.3f} {phi_true:12.3f}\")\n",
    "print(f\"\\nGLS standard errors are LARGER (more honest) because they account\")\n",
    "print(f\"for the fact that correlated observations carry less information.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key takeaways:\n",
    "\n",
    "1. **The coefficient estimates** may be similar, but the **standard errors** from GLS are larger — and more honest. OLS is overconfident.\n",
    "2. **GLS correctly recovered the autocorrelation** ($\\hat\\phi \\approx 0.6$), learning from the data that this semester's GPA is correlated with last semester's.\n",
    "3. **AIC is lower for GLS**, confirming that accounting for correlation gives a better-fitting model.\n",
    "4. A treatment effect that OLS might call \"statistically significant\" could turn out to be non-significant under GLS — or vice versa. Getting this right matters for real decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Correlation Structures: How Data Points Talk to Each Other\n",
    "\n",
    "The key insight of GLS is that the correlation between observations is not a nuisance to be ignored — it's a **structure to be modeled**. Different structures encode different beliefs about *how* observations within a group relate to each other.\n",
    "\n",
    "### Compound Symmetry (Exchangeable)\n",
    "\n",
    "The simplest structure: any two observations from the same student are equally correlated, regardless of how far apart in time they are.\n",
    "\n",
    "$$\\text{Cor}(Y_{i,t}, Y_{i,s}) = \\rho \\quad \\text{for all } t \\neq s$$\n",
    "\n",
    "The correlation matrix for 4 semesters looks like:\n",
    "\n",
    "$$R = \\begin{pmatrix} 1 & \\rho & \\rho & \\rho \\\\ \\rho & 1 & \\rho & \\rho \\\\ \\rho & \\rho & 1 & \\rho \\\\ \\rho & \\rho & \\rho & 1 \\end{pmatrix}$$\n",
    "\n",
    "This says: your GPA in semester 1 is just as correlated with semester 4 as with semester 2. That might be reasonable for some data (e.g., test-retest reliability), but for longitudinal data it's often too restrictive — nearby time points are usually more correlated than distant ones (Snijders & Bosker, 2012, Sec. 15.1.1).\n",
    "\n",
    "### AR(1): Autoregressive\n",
    "\n",
    "A more realistic structure for time-ordered data: correlation decays with distance.\n",
    "\n",
    "$$\\text{Cor}(Y_{i,t}, Y_{i,s}) = \\phi^{|t - s|}$$\n",
    "\n",
    "With $\\phi = 0.6$ and 4 semesters:\n",
    "\n",
    "$$R = \\begin{pmatrix} 1 & 0.6 & 0.36 & 0.216 \\\\ 0.6 & 1 & 0.6 & 0.36 \\\\ 0.36 & 0.6 & 1 & 0.6 \\\\ 0.216 & 0.36 & 0.6 & 1 \\end{pmatrix}$$\n",
    "\n",
    "Adjacent semesters have correlation 0.6, but semesters 1 and 4 have correlation only $0.6^3 = 0.216$. This captures the intuition that recent semesters are more predictive of your current GPA than distant ones.\n",
    "\n",
    "As Snijders and Bosker (2012, Sec. 15.3) describe, autocorrelated residuals arise naturally in longitudinal data. When the random intercept and slope model doesn't fully capture the within-individual dynamics, the level-one residuals often show a pattern where a positive deviation in one period tends to be followed by another positive deviation — exactly the AR(1) pattern.\n",
    "\n",
    "### Unstructured: Let the Data Decide\n",
    "\n",
    "What if the true correlation doesn't follow any simple pattern? Maybe semesters 2 and 3 are highly correlated (both fall/spring of the same academic year) but semesters 1 and 3 are weakly correlated (different academic years). A rigid structure like AR(1) can't capture this.\n",
    "\n",
    "The **unstructured** (or **general**) correlation matrix estimates every pairwise correlation freely:\n",
    "\n",
    "$$R = \\begin{pmatrix} 1 & r_{12} & r_{13} & r_{14} \\\\ r_{12} & 1 & r_{23} & r_{24} \\\\ r_{13} & r_{23} & 1 & r_{34} \\\\ r_{14} & r_{24} & r_{34} & 1 \\end{pmatrix}$$\n",
    "\n",
    "Each $r_{ts}$ is estimated separately from the data. This is the most flexible approach — no pattern is assumed. The trade-off is that it requires estimating $d(d-1)/2$ parameters (6 for 4 time points), which means you need enough data.\n",
    "\n",
    "This is what Snijders and Bosker (2012, Sec. 15.1.3) call the **fully multivariate model**, where the covariance matrix is left \"completely free.\" It serves as a benchmark: if a simpler structure (like AR(1) or compound symmetry) fits almost as well, prefer the simpler model. If not, the unstructured model reveals patterns that simpler models miss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Why Unstructured Correlations Matter\n",
    "\n",
    "You might wonder: \"Can't I just use a mixed effects model (like R's `lme4`) and call it a day?\" Mixed effects models handle grouped data by adding random intercepts and slopes. A random intercept, for instance, gives each student their own baseline GPA level, which induces a compound symmetry correlation structure in the observations.\n",
    "\n",
    "But here's the limitation: **random intercepts produce only compound symmetry**, and **random intercepts + slopes produce a specific structured correlation** that increases (or decreases) in a rigid way over time. These models capture *some* correlation patterns, but not all.\n",
    "\n",
    "Consider a real scenario from educational research:\n",
    "- Semesters 1-2 are highly correlated ($r = 0.7$) because students are adjusting to college\n",
    "- Semesters 3-4 are highly correlated ($r = 0.65$) because students have settled into their major\n",
    "- But semesters 2-3 are only weakly correlated ($r = 0.3$) because something happened in between — maybe students changed majors, or went abroad\n",
    "\n",
    "No random effects structure will capture this pattern well. You need an **unstructured correlation** to let the data reveal this non-monotonic pattern.\n",
    "\n",
    "Let's demonstrate this with `python-gls`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from python_gls.correlation import CorAR1, CorCompSymm, CorSymm\n",
    "\n",
    "# Fit three models with different correlation structures\n",
    "\n",
    "# 1. Compound Symmetry\n",
    "result_cs = GLS.from_formula(\n",
    "    \"gpa ~ study_group + course_load\",\n",
    "    data=df,\n",
    "    correlation=CorCompSymm(),\n",
    "    groups=\"student\",\n",
    ").fit()\n",
    "\n",
    "# 2. AR(1)\n",
    "result_ar1 = GLS.from_formula(\n",
    "    \"gpa ~ study_group + course_load\",\n",
    "    data=df,\n",
    "    correlation=CorAR1(),\n",
    "    groups=\"student\",\n",
    ").fit()\n",
    "\n",
    "# 3. Unstructured (fully free)\n",
    "result_sym = GLS.from_formula(\n",
    "    \"gpa ~ study_group + course_load\",\n",
    "    data=df,\n",
    "    correlation=CorSymm(),\n",
    "    groups=\"student\",\n",
    ").fit()\n",
    "\n",
    "print(\"Model Comparison: Which correlation structure fits best?\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Model':25s} {'AIC':>10s} {'BIC':>10s} {'Log-Lik':>10s} {'# Params':>10s}\")\n",
    "print(\"-\" * 65)\n",
    "print(f\"{'OLS (none)':25s} {result_ols.aic:10.1f} {result_ols.bic:10.1f} {result_ols.loglik:10.1f} {'0':>10s}\")\n",
    "print(f\"{'Compound Symmetry':25s} {result_cs.aic:10.1f} {result_cs.bic:10.1f} {result_cs.loglik:10.1f} {'1 (rho)':>10s}\")\n",
    "print(f\"{'AR(1)':25s} {result_ar1.aic:10.1f} {result_ar1.bic:10.1f} {result_ar1.loglik:10.1f} {'1 (phi)':>10s}\")\n",
    "print(f\"{'Unstructured':25s} {result_sym.aic:10.1f} {result_sym.bic:10.1f} {result_sym.loglik:10.1f} {'6 (r_ij)':>10s}\")\n",
    "print(f\"\\nLower AIC = better fit (penalizing for complexity)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What did each model learn?\n",
    "print(\"Estimated correlation parameters:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nCompound Symmetry: rho = {result_cs.correlation_params[0]:.3f}\")\n",
    "print(f\"  (All pairs have the same correlation)\")\n",
    "\n",
    "print(f\"\\nAR(1): phi = {result_ar1.correlation_params[0]:.3f}\")\n",
    "print(f\"  Implied correlations:\")\n",
    "phi = result_ar1.correlation_params[0]\n",
    "print(f\"    Lag 1 (adjacent semesters): {phi:.3f}\")\n",
    "print(f\"    Lag 2 (2 semesters apart):  {phi**2:.3f}\")\n",
    "print(f\"    Lag 3 (3 semesters apart):  {phi**3:.3f}\")\n",
    "\n",
    "print(f\"\\nUnstructured: each pair estimated freely\")\n",
    "# Reconstruct the correlation matrix from CorSymm\n",
    "print(f\"  Parameters: {result_sym.correlation_params}\")\n",
    "print(f\"  (These are the unique pairwise correlations)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The unstructured model is the most honest: it doesn't force a pattern on the correlations. When the true generating process is AR(1), the AR(1) model will fit best (lowest AIC with fewest parameters). But when the true correlation has an irregular structure — as it often does in real data — the unstructured model is the only one that can capture it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. The Math Behind GLS (Accessible Version)\n",
    "\n",
    "OLS finds coefficients by minimizing the sum of squared errors:\n",
    "\n",
    "$$\\hat{\\beta}_{\\text{OLS}} = (X'X)^{-1}X'y$$\n",
    "\n",
    "GLS does the same, but it **weights** the observations by the inverse of their covariance structure:\n",
    "\n",
    "$$\\hat{\\beta}_{\\text{GLS}} = (X'\\Omega^{-1}X)^{-1}X'\\Omega^{-1}y$$\n",
    "\n",
    "where $\\Omega$ is the covariance matrix that encodes the correlations. Think of it this way:\n",
    "- Observations that are highly correlated with others get **less weight** (they provide redundant information)\n",
    "- Observations that are relatively independent get **more weight** (they provide unique information)\n",
    "\n",
    "The covariance matrix has a specific structure (Snijders & Bosker, 2012, Ch. 15):\n",
    "\n",
    "$$\\Omega_g = \\sigma^2 \\cdot A_g^{1/2} \\, R_g \\, A_g^{1/2}$$\n",
    "\n",
    "where:\n",
    "- $R_g$ is the **correlation matrix** (AR(1), compound symmetry, unstructured, etc.)\n",
    "- $A_g$ is a diagonal matrix of **variance weights** (for heteroscedasticity)\n",
    "- $\\sigma^2$ is the overall residual variance\n",
    "\n",
    "The key innovation in GLS is that $R_g$ and $A_g$ are **estimated from the data** via maximum likelihood (ML) or restricted maximum likelihood (REML). You don't need to know the correlation in advance — the algorithm discovers it.\n",
    "\n",
    "### ML vs REML\n",
    "\n",
    "Two estimation methods are available:\n",
    "- **ML** (Maximum Likelihood): tends to underestimate variance (divides by $N$)\n",
    "- **REML** (Restricted Maximum Likelihood): corrects for the bias (divides by $N - k$), where $k$ is the number of fixed effects\n",
    "\n",
    "REML is the default in both R's `nlme::gls()` and in `python-gls`, and is generally preferred for estimating variance parameters (Snijders & Bosker, 2012, Sec. 4.7)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Beyond Correlation: Heteroscedasticity\n",
    "\n",
    "Sometimes the *variance* of the errors isn't constant either. For example:\n",
    "- GPA variability might be larger for students with heavier course loads (more ways to do well or poorly)\n",
    "- Measurement precision might differ between online and in-person semesters\n",
    "\n",
    "GLS can handle this too, by modeling the variance as a function of a covariate or group membership. `python-gls` provides six variance functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from python_gls.variance import VarIdent\n",
    "\n",
    "# What if the variance differs between study group and non-study-group students?\n",
    "result_hetero = GLS.from_formula(\n",
    "    \"gpa ~ study_group + course_load\",\n",
    "    data=df,\n",
    "    correlation=CorAR1(),\n",
    "    variance=VarIdent(\"study_group\"),  # Allow different variance per group\n",
    "    groups=\"student\",\n",
    ").fit()\n",
    "\n",
    "print(\"GLS with AR(1) + Group-specific Variance\")\n",
    "print(\"=\" * 50)\n",
    "print(result_hetero.summary())\n",
    "print(f\"\\nVariance ratio parameter: {result_hetero.variance_params}\")\n",
    "print(f\"(A value near 1.0 means variances are similar across groups)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Introducing `python-gls`: GLS for Python, Finally\n",
    "\n",
    "### The Gap\n",
    "\n",
    "For over 25 years, R users have had `nlme::gls()` (Pinheiro & Bates, 2000) — a function that estimates correlation and variance structures from data automatically. It has been the workhorse for longitudinal data analysis in biostatistics, social science, and beyond.\n",
    "\n",
    "Python, despite its dominance in data science, has had **no equivalent**. The `statsmodels.GLS` class requires you to supply a pre-computed covariance matrix — you have to know $\\Omega$ in advance, which defeats the purpose. There has been no way to say \"I have AR(1) correlated errors; please estimate $\\phi$ for me.\"\n",
    "\n",
    "**`python-gls` fills this gap.** It is the first Python library to provide automatic estimation of correlation and variance structures via ML/REML, matching R's `nlme::gls()` behavior.\n",
    "\n",
    "### Features at a Glance\n",
    "\n",
    "| Feature | Details |\n",
    "|---|---|\n",
    "| **11 correlation structures** | AR(1), ARMA(p,q), continuous-time AR(1), compound symmetry, unstructured, exponential, Gaussian, linear, rational quadratic, spherical |\n",
    "| **6 variance functions** | Group-specific, power, exponential, constant-power, fixed, combined |\n",
    "| **Estimation** | ML and REML, with profile likelihood optimization |\n",
    "| **R-style formulas** | `\"y ~ x1 + C(group) * time\"` — same syntax you'd use in R |\n",
    "| **Parallel computation** | `n_jobs=-1` distributes group-level operations across CPU cores |\n",
    "| **Validated against R** | Comprehensive test suite comparing output to R's `nlme::gls()` |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance: Parallel GLS\n",
    "\n",
    "Real datasets can be large — thousands of groups, each with dozens of time points. `python-gls` includes several optimizations to make GLS fast:\n",
    "\n",
    "1. **Analytic inverses** for AR(1) and compound symmetry matrices — $O(m)$ instead of $O(m^3)$\n",
    "2. **Batched NumPy operations** for balanced panels — a single matrix multiply across all groups\n",
    "3. **Block-diagonal inversion** — $O(n \\cdot m^3)$ instead of $O(N^3)$\n",
    "4. **Thread-level parallelism** — pass `n_jobs=-1` to distribute work across CPU cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Generate a larger dataset to see parallelism benefits\n",
    "np.random.seed(123)\n",
    "n_large = 500\n",
    "n_t = 8\n",
    "N_large = n_large * n_t\n",
    "\n",
    "df_large = pd.DataFrame({\n",
    "    'y': np.random.randn(N_large),\n",
    "    'x': np.random.randn(N_large),\n",
    "    'group': np.repeat(np.arange(n_large), n_t),\n",
    "})\n",
    "# Add a real signal\n",
    "df_large['y'] = 1.0 + 0.5 * df_large['x'] + df_large['y'] * 0.5\n",
    "\n",
    "# Sequential\n",
    "t0 = time.time()\n",
    "r_seq = GLS.from_formula(\"y ~ x\", data=df_large, correlation=CorAR1(), groups=\"group\").fit(n_jobs=1)\n",
    "t_seq = time.time() - t0\n",
    "\n",
    "# Parallel\n",
    "t0 = time.time()\n",
    "r_par = GLS.from_formula(\"y ~ x\", data=df_large, correlation=CorAR1(), groups=\"group\").fit(n_jobs=-1)\n",
    "t_par = time.time() - t0\n",
    "\n",
    "print(f\"Dataset: {N_large} observations, {n_large} groups, {n_t} time points\")\n",
    "print(f\"Sequential (n_jobs=1):  {t_seq:.2f}s\")\n",
    "print(f\"Parallel   (n_jobs=-1): {t_par:.2f}s\")\n",
    "print(f\"Speedup: {t_seq/t_par:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Putting It All Together: A Complete Workflow\n",
    "\n",
    "Here's the recommended workflow for analyzing longitudinal or clustered data with `python-gls`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Start with OLS as a baseline\n",
    "result_ols = GLS.from_formula(\"gpa ~ study_group + course_load\", data=df).fit()\n",
    "\n",
    "# Step 2: Try different correlation structures\n",
    "result_cs  = GLS.from_formula(\"gpa ~ study_group + course_load\", data=df,\n",
    "                               correlation=CorCompSymm(), groups=\"student\").fit()\n",
    "result_ar1 = GLS.from_formula(\"gpa ~ study_group + course_load\", data=df,\n",
    "                               correlation=CorAR1(), groups=\"student\").fit()\n",
    "result_sym = GLS.from_formula(\"gpa ~ study_group + course_load\", data=df,\n",
    "                               correlation=CorSymm(), groups=\"student\").fit()\n",
    "\n",
    "# Step 3: Compare models using AIC (lower is better)\n",
    "models = [\n",
    "    (\"OLS\",                result_ols),\n",
    "    (\"Compound Symmetry\",  result_cs),\n",
    "    (\"AR(1)\",              result_ar1),\n",
    "    (\"Unstructured\",       result_sym),\n",
    "]\n",
    "\n",
    "print(\"Step 3: Model Selection via AIC\")\n",
    "print(\"=\" * 50)\n",
    "for name, r in sorted(models, key=lambda x: x[1].aic):\n",
    "    print(f\"  {name:25s}  AIC = {r.aic:8.1f}  {'<-- best' if r.aic == min(m[1].aic for m in models) else ''}\")\n",
    "\n",
    "# Step 4: Interpret the best model\n",
    "best_name, best_result = min(models, key=lambda x: x[1].aic)\n",
    "print(f\"\\nStep 4: Results from best model ({best_name})\")\n",
    "print(\"=\" * 50)\n",
    "print(best_result.summary())\n",
    "print(f\"\\n95% Confidence Intervals:\")\n",
    "print(best_result.conf_int())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Summary\n",
    "\n",
    "### What You've Learned\n",
    "\n",
    "1. **OLS assumes independence** — but real data (especially longitudinal or clustered data) violates this. Ignoring the violation leads to standard errors that are too small and conclusions that are unreliable.\n",
    "\n",
    "2. **Correlation structures** describe how observations within a group are related. Common choices include:\n",
    "   - **Compound symmetry**: all pairs equally correlated (simplest, most restrictive)\n",
    "   - **AR(1)**: correlation decays with time lag (natural for time series)\n",
    "   - **Unstructured**: every pair has its own correlation (most flexible, lets data speak)\n",
    "\n",
    "3. **Unstructured correlations** are especially valuable when the pattern of association between time points doesn't follow a simple formula. While mixed effects models (random intercepts/slopes) impose specific structures, GLS with `CorSymm` can capture arbitrary patterns.\n",
    "\n",
    "4. **GLS estimates these structures from your data** via maximum likelihood — you don't need to know the correlation in advance.\n",
    "\n",
    "5. **Model selection** (AIC/BIC) helps you choose the right level of complexity: start simple, and add complexity only when the data supports it.\n",
    "\n",
    "### `python-gls` in One Paragraph\n",
    "\n",
    "`python-gls` is the first Python library to provide automatic GLS estimation with learned correlation and variance structures — functionality that has been available in R through `nlme::gls()` (Pinheiro & Bates, 2000) for over 25 years but has been missing from the Python ecosystem. It supports 11 correlation structures (including unstructured), 6 variance functions, both ML and REML estimation, R-style formula syntax, and parallel computation for large datasets. It has been validated against R's `nlme::gls()` across 12 diverse test scenarios.\n",
    "\n",
    "Install it with:\n",
    "```bash\n",
    "pip install python-gls\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- Dorman, C. F. (2008). Effects of incorporating spatial autocorrelation into the analysis of species distribution data. *Global Ecology and Biogeography*, 16(2), 129–138.\n",
    "\n",
    "- Hedges, L. V., & Hedberg, E. C. (2007). Intraclass correlation values for planning group-randomized trials in education. *Educational Evaluation and Policy Analysis*, 29(1), 60–87.\n",
    "\n",
    "- Pinheiro, J. C., & Bates, D. M. (2000). *Mixed-Effects Models in S and S-PLUS*. Springer.\n",
    "\n",
    "- Pinheiro, J. C., & Bates, D. M. (1996). Unconstrained parametrizations for variance-covariance matrices. *Statistics and Computing*, 6(3), 289–296.\n",
    "\n",
    "- Snijders, T. A. B., & Bosker, R. J. (2012). *Multilevel Analysis: An Introduction to Basic and Advanced Multilevel Modeling* (2nd ed.). SAGE Publications.\n",
    "  - Ch. 2: Dependence as a nuisance vs. an interesting phenomenon\n",
    "  - Ch. 3, Sec. 3.3–3.4: Intraclass correlation and design effects\n",
    "  - Ch. 15, Sec. 15.1: Fixed occasion designs, compound symmetry\n",
    "  - Ch. 15, Sec. 15.1.3: The fully multivariate (unstructured) model\n",
    "  - Ch. 15, Sec. 15.3: Autocorrelated residuals"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
